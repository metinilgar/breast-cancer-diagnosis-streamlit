import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os
import sys
import time
from datetime import datetime

# ModÃ¼lleri import et
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from utils.preprocessing import load_data, preprocess_data
from utils.model_utils import (
    train_classical_model, train_neural_network, evaluate_model, 
    save_model, plot_confusion_matrix, plot_roc_curve, calculate_feature_importance
)
from utils.visualizations import plot_feature_importance, plot_learning_curve, plot_metrics_comparison

# Sayfa konfigÃ¼rasyonu
st.set_page_config(
    page_title="Model EÄŸitimi",
    page_icon="ğŸ§ ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Ana baÅŸlÄ±k
st.title("ğŸ§  Kendi Modelinizi EÄŸitin")

# Sidebar - Veri YÃ¼kleme SeÃ§enekleri
st.sidebar.header("Veri SeÃ§enekleri")

data_source = st.sidebar.radio(
    "Veri KaynaÄŸÄ±",
    ["Ã–rnek Veri Seti", "Kendi Verinizi YÃ¼kleyin"]
)

df = None

if data_source == "Ã–rnek Veri Seti":
    # VarsayÄ±lan veri setini yÃ¼kle
    df = load_data()
    st.sidebar.success("Wisconsin Breast Cancer veri seti yÃ¼klendi!")
    
else:
    uploaded_file = st.sidebar.file_uploader("CSV veya Excel dosyasÄ± yÃ¼kleyin", type=["csv", "xlsx", "xls"])
    
    if uploaded_file is not None:
        try:
            # Dosya uzantÄ±sÄ±nÄ± kontrol et
            if uploaded_file.name.endswith('.csv'):
                df = pd.read_csv(uploaded_file)
            else:
                df = pd.read_excel(uploaded_file)
            
            st.sidebar.success(f"{uploaded_file.name} baÅŸarÄ±yla yÃ¼klendi!")
        except Exception as e:
            st.sidebar.error(f"Hata: {e}")
    else:
        st.sidebar.info("LÃ¼tfen bir dosya yÃ¼kleyin veya Ã¶rnek veri setini kullanÄ±n.")
        # Ã–rnek veri setini yÃ¼kle
        df = load_data()
        st.sidebar.success("Wisconsin Breast Cancer veri seti yÃ¼klendi!")

# Ana sayfa iÃ§eriÄŸi
if df is not None:
    # Veri bilgilerini gÃ¶ster
    st.subheader("Veri Seti Ã–nizleme")
    st.dataframe(df.head(), use_container_width=True)
    
    st.markdown(f"**Veri Boyutu:** {df.shape[0]} satÄ±r, {df.shape[1]} sÃ¼tun")
    
    # Eksik deÄŸerlerin %40'Ä±ndan fazlasÄ± olan sÃ¼tunlarÄ± gÃ¶ster
    missing_percentage = df.isnull().mean()
    problematic_cols = missing_percentage[missing_percentage > 0.4].index.tolist()
    
    if problematic_cols:
        st.warning(f"âš ï¸ AÅŸaÄŸÄ±daki sÃ¼tunlarda %40'tan fazla eksik veri var ve Ã¶n iÅŸleme sÄ±rasÄ±nda silinecek (hedef sÃ¼tun hariÃ§):\n{', '.join(problematic_cols)}")
    
    # Veri Ã¶n iÅŸleme ayarlarÄ±
    st.header("Veri Ã–n Ä°ÅŸleme AyarlarÄ±")
    
    # Hedef sÃ¼tun seÃ§imi
    # EÄŸer Ã¶rnek veri seti kullanÄ±lÄ±yorsa varsayÄ±lan hedef sÃ¼tun "target" olacak
    default_target_index = 0  # VarsayÄ±lan olarak boÅŸ seÃ§enek
    
    if data_source == "Ã–rnek Veri Seti" and "target" in df.columns:
        # Ã–rnek veri seti iÃ§in "target" sÃ¼tununu seÃ§ili hale getir
        default_target_index = df.columns.tolist().index("target") + 1  # +1 Ã§Ã¼nkÃ¼ boÅŸ seÃ§enek ekledik
    
    target_column = st.selectbox(
        "Hedef SÃ¼tunu SeÃ§in",
        options=[""] + df.columns.tolist(),  # BoÅŸ seÃ§enek eklendi
        index=default_target_index  # Ã–rnek veri setiyse target, deÄŸilse boÅŸ
    )
    
    # Hedef sÃ¼tun seÃ§ilmediÄŸinde uyarÄ± gÃ¶ster
    if not target_column:
        st.warning("âš ï¸ LÃ¼tfen hedef sÃ¼tunu seÃ§in. Aksi takdirde varsayÄ±lan hedef sÃ¼tun kullanÄ±lacak!")
    # Hedef sÃ¼tun problematik sÃ¼tunlar arasÄ±ndaysa uyarÄ± gÃ¶ster
    elif target_column in problematic_cols:
        st.error(f"âš ï¸ SeÃ§ilen hedef sÃ¼tun '{target_column}' eksik verilerin Ã§ok olduÄŸu bir sÃ¼tun, ancak Ã¶n iÅŸleme sÄ±rasÄ±nda korunacak.")
    
    # EÄŸitimden Ã§Ä±karÄ±lacak sÃ¼tunlarÄ± seÃ§me
    st.subheader("EÄŸitimden Ã‡Ä±karÄ±lacak SÃ¼tunlar")
    
    # Hedef sÃ¼tun dÄ±ÅŸÄ±ndaki tÃ¼m sÃ¼tunlarÄ± gÃ¶ster
    available_columns = [col for col in df.columns if col != target_column]
    
    columns_to_drop = st.multiselect(
        "EÄŸitimden Ã‡Ä±karÄ±lacak SÃ¼tunlarÄ± SeÃ§in",
        options=available_columns,
        default=[],
        help="EÄŸitim sÃ¼recine dahil etmek istemediÄŸiniz sÃ¼tunlarÄ± seÃ§in. Bu sÃ¼tunlar veri setinden Ã§Ä±karÄ±lacaktÄ±r."
    )
    
    if columns_to_drop:
        st.info(f"AÅŸaÄŸÄ±daki sÃ¼tunlar eÄŸitimden Ã§Ä±karÄ±lacak: {', '.join(columns_to_drop)}")
    
    # Test seti oranÄ±
    test_size = st.slider("Test Seti OranÄ±", min_value=0.1, max_value=0.5, value=0.2, step=0.05)
    
    # Scaler seÃ§imi
    st.subheader("Veri Ã–lÃ§eklendirme AyarlarÄ±")
    scaler_type = st.selectbox(
        "Veri Ã–lÃ§eklendirme YÃ¶ntemi",
        options=["StandardScaler", "MinMaxScaler", "NoScaler"],
        index=0,
        help="StandardScaler: Veriyi ortalamasÄ± 0, standart sapmasÄ± 1 olacak ÅŸekilde Ã¶lÃ§eklendirir. MinMaxScaler: Veriyi 0-1 aralÄ±ÄŸÄ±na Ã¶lÃ§eklendirir. NoScaler: Herhangi bir Ã¶lÃ§eklendirme yapmaz."
    )
    
    # SeÃ§ilen scaler hakkÄ±nda bilgi
    if scaler_type == "StandardScaler":
        st.info("**StandardScaler**: Veriyi standartlaÅŸtÄ±rÄ±r (z-score normalizasyonu). Her Ã¶zellik iÃ§in ortalama 0, standart sapma 1 olur.")
    elif scaler_type == "MinMaxScaler":
        st.info("**MinMaxScaler**: Veriyi 0-1 aralÄ±ÄŸÄ±na Ã¶lÃ§eklendirir. Minimum deÄŸer 0, maksimum deÄŸer 1 olur.")
    elif scaler_type == "NoScaler":
        st.warning("**Scaler Yok**: Herhangi bir Ã¶lÃ§eklendirme yapÄ±lmaz. Veriler orijinal Ã¶lÃ§eÄŸinde kalÄ±r. Bu durum bazÄ± algoritmalarÄ±n performansÄ±nÄ± olumsuz etkileyebilir.")
    
    # Model seÃ§imi
    st.header("Model SeÃ§imi")
    
    model_type = st.selectbox(
        "Model TÃ¼rÃ¼",
        options=["Klasik Makine Ã–ÄŸrenmesi", "Yapay Sinir AÄŸÄ±"]
    )
    
    # Klasik model tÃ¼rleri
    if model_type == "Klasik Makine Ã–ÄŸrenmesi":
        selected_model = st.selectbox(
            "Algoritma",
            options=["Random Forest", "Logistic Regression", "SVM", "KNN", "Decision Tree", "Gradient Boosting", "MLP"]
        )
        
        # Model parametreleri - her model iÃ§in Ã¶zel parametreler
        st.subheader("Model Parametreleri")
        
        if selected_model == "Random Forest":
            n_estimators = st.slider("AÄŸaÃ§ SayÄ±sÄ±", min_value=10, max_value=500, value=100, step=10)
            max_depth = st.slider("Maksimum Derinlik", min_value=2, max_value=30, value=10, step=1)
            model_params = {
                "n_estimators": n_estimators,
                "max_depth": max_depth,
                "random_state": 42
            }
            model_code = "random_forest"
            
        elif selected_model == "Logistic Regression":
            C = st.select_slider("DÃ¼zenlileÅŸtirme Parametresi (C)", options=[0.001, 0.01, 0.1, 1.0, 10.0, 100.0], value=1.0)
            solver = st.selectbox("Ã‡Ã¶zÃ¼cÃ¼", options=["liblinear", "lbfgs", "newton-cg", "sag"], index=0)
            model_params = {
                "C": C,
                "solver": solver,
                "max_iter": 1000,
                "random_state": 42
            }
            model_code = "logistic_regression"
            
        elif selected_model == "SVM":
            C = st.select_slider("DÃ¼zenlileÅŸtirme Parametresi (C)", options=[0.001, 0.01, 0.1, 1.0, 10.0, 100.0], value=1.0)
            kernel = st.selectbox("Ã‡ekirdek Fonksiyonu", options=["linear", "rbf", "poly", "sigmoid"], index=1)
            gamma = st.select_slider("Gamma", options=["scale", "auto", 0.001, 0.01, 0.1, 1.0], value="scale")
            model_params = {
                "C": C,
                "kernel": kernel,
                "gamma": gamma,
                "random_state": 42
            }
            model_code = "svm"
            
        elif selected_model == "KNN":
            n_neighbors = st.slider("KomÅŸu SayÄ±sÄ±", min_value=1, max_value=20, value=5, step=1)
            weights = st.selectbox("AÄŸÄ±rlÄ±k Fonksiyonu", options=["uniform", "distance"], index=0)
            model_params = {
                "n_neighbors": n_neighbors,
                "weights": weights
            }
            model_code = "knn"
            
        elif selected_model == "Decision Tree":
            max_depth = st.slider("Maksimum Derinlik", min_value=2, max_value=30, value=5, step=1)
            criterion = st.selectbox("Kriter", options=["gini", "entropy"], index=0)
            model_params = {
                "max_depth": max_depth,
                "criterion": criterion,
                "random_state": 42
            }
            model_code = "decision_tree"
            
        elif selected_model == "Gradient Boosting":
            n_estimators = st.slider("AÄŸaÃ§ SayÄ±sÄ±", min_value=10, max_value=500, value=100, step=10)
            learning_rate = st.select_slider("Ã–ÄŸrenme OranÄ±", options=[0.001, 0.01, 0.05, 0.1, 0.2, 0.5], value=0.1)
            max_depth = st.slider("Maksimum Derinlik", min_value=2, max_value=10, value=3, step=1)
            model_params = {
                "n_estimators": n_estimators,
                "learning_rate": learning_rate,
                "max_depth": max_depth,
                "random_state": 42
            }
            model_code = "gradient_boosting"
            
        elif selected_model == "MLP":
            hidden_layer_sizes = st.text_input("Gizli Katman BoyutlarÄ± (virgÃ¼lle ayrÄ±lmÄ±ÅŸ)", value="100,50")
            activation = st.selectbox("Aktivasyon Fonksiyonu", options=["relu", "tanh", "logistic"], index=0)
            max_iter = st.slider("Maksimum Ä°terasyon", min_value=100, max_value=1000, value=300, step=100)
            
            # Gizli katman boyutlarÄ±nÄ± tuple'a dÃ¶nÃ¼ÅŸtÃ¼r
            hidden_layer_sizes = tuple(map(int, hidden_layer_sizes.split(',')))
            
            model_params = {
                "hidden_layer_sizes": hidden_layer_sizes,
                "activation": activation,
                "max_iter": max_iter,
                "random_state": 42
            }
            model_code = "mlp"
            
    # Yapay Sinir AÄŸÄ± modeli
    else:
        st.subheader("Yapay Sinir AÄŸÄ± Parametreleri")
        
        # Katman sayÄ±sÄ±
        n_layers = st.slider("Gizli Katman SayÄ±sÄ±", min_value=1, max_value=5, value=2, step=1)
        
        # Katman boyutlarÄ±
        layers = []
        for i in range(n_layers):
            layer_size = st.slider(f"{i+1}. Katman NÃ¶ron SayÄ±sÄ±", min_value=8, max_value=256, value=64 // (2**min(i, 2)), step=8)
            layers.append(layer_size)
        
        # Aktivasyon fonksiyonu
        activation = st.selectbox("Aktivasyon Fonksiyonu", options=["relu", "tanh", "sigmoid"], index=0)
        
        # Dropout oranÄ±
        dropout_rate = st.slider("Dropout OranÄ±", min_value=0.0, max_value=0.5, value=0.2, step=0.05)
        
        # Optimizer
        optimizer = st.selectbox("Optimizer", options=["adam", "sgd", "rmsprop"], index=0)
        
        # Batch size ve epoch
        batch_size = st.slider("Batch Size", min_value=8, max_value=128, value=32, step=8)
        epochs = st.slider("Epoch SayÄ±sÄ±", min_value=10, max_value=200, value=50, step=10)
        
        model_params = {
            "layers": layers,
            "activation": activation,
            "dropout_rate": dropout_rate,
            "optimizer": optimizer,
            "batch_size": batch_size,
            "epochs": epochs
        }
        
        model_code = "neural_network"
        selected_model = "Neural Network"
    
    # Model adÄ±
    model_name = st.text_input("Model AdÄ±", value=f"{selected_model.lower().replace(' ', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
    
    # EÄŸitim butonu
    train_button = st.button("Modeli EÄŸit", type="primary")
    
    if train_button:
        st.info("Model eÄŸitimi baÅŸlatÄ±lÄ±yor... LÃ¼tfen bekleyin.")
        
        # Veri Ã¶n iÅŸleme
        try:
            with st.spinner("Veriler Ã¶n iÅŸleniyor..."):
                # KullanÄ±cÄ±nÄ±n seÃ§tiÄŸi Ã§Ä±karÄ±lacak sÃ¼tunlarÄ± preprocessing fonksiyonuna ilet
                X_train, X_test, y_train, y_test, scaler, warnings = preprocess_data(
                    df, 
                    target_column, 
                    test_size,
                    columns_to_drop=columns_to_drop,
                    scaler_type=scaler_type
                )
                
                # EÄŸitim ve test seti bilgisi
                st.success(f"Veriler Ã¶n iÅŸlendi. EÄŸitim seti: {X_train.shape[0]} Ã¶rnek, Test seti: {X_test.shape[0]} Ã¶rnek")
                
                # Silinen sÃ¼tunlar hakkÄ±nda bilgi
                if columns_to_drop:
                    st.info(f"SeÃ§tiÄŸiniz {len(columns_to_drop)} sÃ¼tun eÄŸitimden Ã§Ä±karÄ±ldÄ±: {', '.join(columns_to_drop)}")
                
                # Hedef sÃ¼tun seÃ§ilmediÄŸinde veya korunduÄŸunda uyarÄ±lar
                if warnings["target_not_selected"]:
                    st.warning(f"âš ï¸ Hedef sÃ¼tun seÃ§ilmediÄŸi iÃ§in varsayÄ±lan sÃ¼tun '{warnings['target_column_used']}' kullanÄ±ldÄ±.")
                
                if warnings["target_protected_from_deletion"]:
                    st.error(f"âš ï¸ SeÃ§ilen hedef sÃ¼tun '{target_column}' eksik verilerin Ã§ok olduÄŸu bir sÃ¼tun, ancak Ã¶n iÅŸleme sÄ±rasÄ±nda korundu.")
                
                # EÄŸitimde kullanÄ±lan Ã¶zellik isimlerini belirle (Ã¶zellik Ã¶nemi iÃ§in gerekli)
                if isinstance(df, pd.DataFrame):
                    all_features = [col for col in df.columns if col != target_column]
                    # Ã‡Ä±karÄ±lan sÃ¼tunlarÄ± hariÃ§ tut
                    if columns_to_drop:
                        feature_names = [col for col in all_features if col not in columns_to_drop]
                    else:
                        feature_names = all_features
                else:
                    feature_names = [f"feature_{i}" for i in range(X_train.shape[1])]
            
            # Model eÄŸitimi
            with st.spinner(f"{selected_model} modeli eÄŸitiliyor..."):
                start_time = time.time()
                
                if model_code == "neural_network":
                    model, model_info = train_neural_network(X_train, y_train, X_train.shape[1], model_params)
                    model_type_str = "neural_network"
                else:
                    model, model_info = train_classical_model(X_train, y_train, model_code, model_params)
                    model_type_str = "classical"
                
                training_time = time.time() - start_time
                
                st.success(f"Model baÅŸarÄ±yla eÄŸitildi. EÄŸitim sÃ¼resi: {training_time:.2f} saniye")
            
            # Model deÄŸerlendirme
            with st.spinner("Model deÄŸerlendiriliyor..."):
                metrics = evaluate_model(model, X_test, y_test, model_type_str)
                
                # DeÄŸerlendirme metriklerini gÃ¶ster
                st.subheader("Model Performans Metrikleri")
                
                col1, col2, col3, col4 = st.columns(4)
                col1.metric("DoÄŸruluk (Accuracy)", f"{metrics['accuracy']:.4f}")
                col2.metric("Kesinlik (Precision)", f"{metrics['precision']:.4f}")
                col3.metric("Hassasiyet (Recall)", f"{metrics['recall']:.4f}")
                col4.metric("F1 Skoru", f"{metrics['f1_score']:.4f}")
                
                # ROC-AUC deÄŸeri
                if metrics['roc_auc'] is not None:
                    st.metric("ROC AUC Skoru", f"{metrics['roc_auc']:.4f}")
                
                # KarmaÅŸÄ±klÄ±k matrisi ve ROC eÄŸrisi
                col1, col2 = st.columns(2)
                
                with col1:
                    st.subheader("KarmaÅŸÄ±klÄ±k Matrisi")
                    cm_fig = plot_confusion_matrix(np.array(metrics['confusion_matrix']))
                    st.pyplot(cm_fig)
                    
                with col2:
                    # ROC eÄŸrisi iÃ§in tahmin olasÄ±lÄ±klarÄ± gerekiyor
                    st.subheader("ROC EÄŸrisi")
                    
                    if model_type_str == "classical":
                        if hasattr(model, "predict_proba"):
                            y_pred_proba = model.predict_proba(X_test)[:, 1]
                        else:
                            # SVC gibi modeller iÃ§in decision_function kullan
                            if hasattr(model, "decision_function"):
                                y_pred_proba = model.decision_function(X_test)
                            else:
                                y_pred_proba = model.predict(X_test)
                    else:  # neural_network
                        y_pred_proba = model.predict(X_test).ravel()
                    
                    roc_fig = plot_roc_curve(y_test, y_pred_proba)
                    st.pyplot(roc_fig)
                
                # Yapay sinir aÄŸÄ± iÃ§in eÄŸitim geÃ§miÅŸi
                if model_code == "neural_network" and "history" in model_info:
                    st.subheader("EÄŸitim GeÃ§miÅŸi")
                    history_fig = plot_learning_curve(model_info["history"])
                    st.pyplot(history_fig)
                
                # Ã–zellik Ã¶nemi (eÄŸer model destekliyorsa)
                if model_code != "neural_network" and model_code not in ["knn"]:
                    st.subheader("Ã–zellik Ã–nemi")
                    
                    # Ã–zellik Ã¶nemini hesapla (yukarÄ±da hesaplanan feature_names kullan)
                    feature_importance = calculate_feature_importance(model, feature_names, model_type_str)
                    
                    # Ã–zellik Ã¶nemini gÃ¶rselleÅŸtir
                    importance_fig = plot_feature_importance(feature_importance)
                    st.pyplot(importance_fig)
                
                # Preprocessing bilgilerini hazÄ±rla
                preprocessing_info = {
                    "columns_to_drop": columns_to_drop or [],
                    "target_column": target_column,
                    "feature_names": feature_names,
                    "test_size": test_size,
                    "scaler_type": scaler_type
                }
                
                # Modeli kaydet
                save_path = save_model(model, model_info, metrics, model_name, directory="models/custom", preprocessing_info=preprocessing_info, scaler=scaler)
                st.success(f"Model baÅŸarÄ±yla kaydedildi: {save_path}")
                
                # Model Ã¶zeti
                st.subheader("Model Ã–zeti")
                
                st.write(f"**Model AdÄ±:** {model_name}")
                st.write(f"**Model TÃ¼rÃ¼:** {selected_model}")
                st.write(f"**DoÄŸruluk (Accuracy):** {metrics['accuracy']:.4f}")
                st.write(f"**EÄŸitim SÃ¼resi:** {training_time:.2f} saniye")
                
                st.info("""
                **Not:** EÄŸittiÄŸiniz model "models/custom" klasÃ¶rÃ¼ne kaydedildi. 
                Modeli "HazÄ±r Modeller" ve "Tahmin" sayfalarÄ±nda kullanabilirsiniz.
                """)
                
        except Exception as e:
            st.error(f"Model eÄŸitimi sÄ±rasÄ±nda bir hata oluÅŸtu: {str(e)}")
            st.exception(e)
            
else:
    st.error("Veri yÃ¼klenemedi. LÃ¼tfen veri kaynaÄŸÄ±nÄ±zÄ± kontrol edin.")
